{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91MsGMTna_P9"
   },
   "source": [
    "# CBU5201 mini-project submission\n",
    "\n",
    "\n",
    "## What is the problem?\n",
    "\n",
    "This year's mini-project considers the problem of predicting whether a narrated story is true or not. Specifically, you will build a machine learning model that takes as an input an audio recording of **3-5 minutes** of duration and predicts whether the story being narrated is **true or not**. \n",
    "\n",
    "\n",
    "## Which dataset will I use?\n",
    "\n",
    "A total of 100 samples consisting of a complete audio recording, a *Language* attribute and a *Story Type* attribute have been made available for you to build your machine learning model. The audio recordings can be downloaded from:\n",
    "\n",
    "https://github.com/CBU5201Datasets/Deception\n",
    "\n",
    "A CSV file recording the *Language* attribute and *Story Type* of each audio file can be downloaded from:\n",
    "\n",
    "https://github.com/CBU5201Datasets/Deception/blob/main/CBU0521DD_stories_attributes.csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## What will I submit?\n",
    "\n",
    "Your submission will consist of **one single Jupyter notebook** that should include:\n",
    "\n",
    "*   **Text cells**, describing in your own words, rigorously and concisely your approach, each implemented step and the results that you obtain,\n",
    "*   **Code cells**, implementing each step,\n",
    "*   **Output cells**, i.e. the output from each code cell,\n",
    "\n",
    "Your notebook **should have the structure** outlined below. Please make sure that you **run all the cells** and that the **output cells are saved** before submission. \n",
    "\n",
    "Please save your notebook as:\n",
    "\n",
    "* CBU5201_miniproject.ipynb\n",
    "\n",
    "\n",
    "## How will my submission be evaluated?\n",
    "\n",
    "This submission is worth 16 marks. We will value:\n",
    "\n",
    "*   Conciseness in your writing.\n",
    "*   Correctness in your methodology.\n",
    "*   Correctness in your analysis and conclusions.\n",
    "*   Completeness.\n",
    "*   Originality and efforts to try something new.\n",
    "\n",
    "(4 marks are given based on your audio submission from stage 1.)\n",
    "\n",
    "**The final performance of your solutions will not influence your grade**. We will grade your understanding. If you have an good understanding, you will be using the right methodology, selecting the right approaches, assessing correctly the quality of your solutions, sometimes acknowledging that despite your attempts your solutions are not good enough, and critically reflecting on your work to suggest what you could have done differently. \n",
    "\n",
    "Note that **the problem that we are intending to solve is very difficult**. Do not despair if you do not get good results, **difficulty is precisely what makes it interesting** and **worth trying**. \n",
    "\n",
    "## Show the world what you can do \n",
    "\n",
    "Why don't you use **GitHub** to manage your project? GitHub can be used as a presentation card that showcases what you have done and gives evidence of your data science skills, knowledge and experience. **Potential employers are always looking for this kind of evidence**. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------- PLEASE USE THE STRUCTURE BELOW THIS LINE --------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization Improvement of Transformer and DNN Models in Audio Feature Analysis Predicting the Truthfulness of Narrated Stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaGn4ICrfqXZ"
   },
   "source": [
    "# 1 Author\n",
    "\n",
    "**Student Name**:  Songheng Zhan\n",
    "**Student ID**:  221171028\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o38VQkcdKd6k"
   },
   "source": [
    "# 2 Problem formulation\n",
    "\n",
    "Describe the machine learning problem that you want to solve and explain what's interesting about it.\n",
    "\n",
    "### Audio Preprocessing\n",
    "Noise Reduction: Audio recordings often contain background noise, which can significantly affect classification performance. Therefore, effective noise reduction is crucial to improving the quality of the audio data, enabling the model to analyze the content more accurately.\n",
    "\n",
    "Channel Conversion: Converting stereo audio to mono ensures consistency in the input data, simplifying the feature extraction process and avoiding complications arising from differences between audio channels.\n",
    "\n",
    "Volume Normalization: Ensuring that all audio segments have a consistent volume helps mitigate the influence of loudness variations in different recordings, allowing the model to focus more on the content rather than the loudness.\n",
    "\n",
    "### Feature Extraction\n",
    "In this project, I am extracting multiple audio features to enhance the model's performance:\n",
    "\n",
    "Mel-Frequency Cepstral Coefficients (MFCC): This feature effectively captures the spectral information of audio signals and is particularly useful for distinguishing different types of sounds.\n",
    "\n",
    "Chroma Features: These features reflect the harmonic distribution of audio, which aids the model in recognizing pitch variations and is significant for both music and speech analysis.\n",
    "\n",
    "Mel Spectrogram: This representation transforms audio signals into frequency maps, helping the model understand the temporal and spectral characteristics of the sounds.\n",
    "\n",
    "OpenL3 Features: By using features extracted from the OpenL3 model, I provide a high-level representation of the audio signal from a deep learning perspective, further enriching the diversity of features.\n",
    "\n",
    "### Model Design\n",
    "Deep Neural Network (DNN): I designed a DNN model that utilizes multiple fully connected layers, suitable for processing the extracted diverse features. The non-linear activation functions enhance the model’s capability to learn complex patterns.\n",
    "\n",
    "Transformer Model: Additionally, employing a Transformer model takes advantage of its strengths in handling sequential data, particularly through the self-attention mechanism, which captures long-range dependencies in audio signals, improving the model's understanding of the audio content.\n",
    "\n",
    "### Data Handling\n",
    "Data Splitting: In this study, the dataset consists of 80 training samples and 20 testing samples. Dividing the data into training and testing sets ensures effective performance evaluation of the model.\n",
    "\n",
    "### Evaluation Metrics\n",
    "Loss and Accuracy: Continuously monitoring these metrics during training helps assess the learning performance and allows for targeted adjustments, ultimately aiding in enhancing classification performance.\n",
    "\n",
    "### What's Interesting About This Problem\n",
    "Real-World Applications: Audio classification has widespread applications across various fields, such as detecting fraudulent calls, performing sentiment analysis, and enabling voice commands, all of which can significantly enhance user experience and increase the reliability of services.\n",
    "\n",
    "Multidisciplinary Nature: This problem combines knowledge from signal processing, machine learning, and deep learning, making it a challenging research topic that requires a broad skill set.\n",
    "\n",
    "Quality of Data and Preprocessing: The quality of audio data has a substantial impact on model performance. Proper preprocessing can notably improve outcomes, ensuring that the model is trained on high-quality inputs.\n",
    "\n",
    "Complexity of Feature Engineering: Extracting diverse and expressive features is critical for the success of the model, necessitating a deep understanding and exploration of audio signals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPTSuaB9L2jU"
   },
   "source": [
    "# 3 Methodology\n",
    "\n",
    "Describe your methodology. Specifically, describe your training task and validation task, and how model performance is defined (i.e. accuracy, confusion matrix, etc). Any other tasks that might help you build your model should also be described here.\n",
    "### Overview\n",
    "This methodology aims to develop an audio classification model capable of categorizing audio data into \"True Story\" and \"Deceptive Story.\" The entire process will include data preprocessing, feature extraction, model training, and performance evaluation.\n",
    "\n",
    "### Data Collection and Preprocessing\n",
    "**Data Input**: Audio files and label information are obtained by reading a CSV file.\n",
    "**Audio Preprocessing**: The audio files are loaded using an audio processing library, converted to mono, and normalized for loudness. Additionally, a low-pass filter is applied to remove high-frequency noise, and short-time Fourier transform (STFT) is used to process the audio data, followed by noise reduction through spectral subtraction.\n",
    "\n",
    "### Feature Extraction\n",
    "Relevant audio features are extracted, including Mel-frequency cepstral coefficients (MFCC), chroma features, Mel spectrogram features, and embeddings extracted using the OpenL3 model with their mean values computed.\n",
    "\n",
    "### Dataset Construction\n",
    "A training dataset and labels are created. During the processing stage, a progress bar tool is used to display data processing progress while extracting features.\n",
    "\n",
    "### Model Definition and Training\n",
    "**Deep Neural Network (DNN)**\n",
    "A deep neural network model is defined with an input feature dimension that includes multiple audio features, and the model structure includes a hidden layer. The model is trained, and both loss and accuracy are recorded to evaluate its performance.\n",
    "\n",
    "**Transformer Model**\n",
    "A Transformer model is defined, utilizing an embedding layer and a Transformer encoder structure to handle the input features and capture the sequential information of the audio data. This model includes multiple self-attention heads and layers to enhance representation learning capabilities, and the training process is similar to that of the DNN, with loss and accuracy recorded.\n",
    "\n",
    "**Training Process**\n",
    "The dataset is split into training and testing sets, using the Adam optimizer and cross-entropy loss function for model training. During the training process, regular validation is conducted, and the best-performing model is saved.\n",
    "\n",
    "### Validation Task\n",
    "At specific intervals, test loss and accuracy are calculated through forward propagation to evaluate the generalization ability of each model.\n",
    "\n",
    "### Performance Metrics\n",
    "**Accuracy**: Measured by calculating the ratio of correctly predicted instances to the total number of instances, reflecting model performance.\n",
    "**Confusion Matrix**: Provides detailed statistics on true positives, false positives, true negatives, and false negatives.\n",
    "Optional performance metrics include precision, recall, and F1-score, particularly important in the context of imbalanced datasets.\n",
    "\n",
    "### Additional Tasks\n",
    "**Data Augmentation**: During the training phase, data augmentation techniques such as pitch shifting and time stretching can be implemented to enhance model robustness.\n",
    "**Feature Selection**: Monitoring the importance of features to retain those that have the most significant impact on model performance.\n",
    "\n",
    "### Final Model Development and Testing\n",
    "After training and validating different models (including both DNN and Transformer), the best-performing model is identified and tested on a test dataset to confirm the model's real-world efficacy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3BwrtEdLDit"
   },
   "source": [
    "# 4 Implemented ML prediction pipelines\n",
    "\n",
    "Describe the ML prediction pipelines that you will explore. Clearly identify their input and output, stages and format of the intermediate data structures moving from one stage to the next. It's up to you to decide which stages to include in your pipeline. After providing an overview, describe in more detail each one of the stages that you have included in their corresponding subsections (i.e. 4.1 Transformation stage, 4.2 Model stage, 4.3 Ensemble stage).\n",
    "\n",
    "The machine learning prediction pipeline for audio classification consists of three primary stages:\n",
    "\n",
    "Transformation Stage\n",
    "Model Stage\n",
    "Ensemble Stage\n",
    "Each stage is designed to systematically process the audio data, ensuring optimal preparation for modeling and improving overall classification accuracy through ensemble techniques. Below, I will detail the input and output for each stage and describe the intermediate data structures that flow from one stage to the next.\n",
    "\n",
    "### 4.1 Transformation Stage\n",
    "**Input**\n",
    "Raw Audio Files: The initial input consists of audio recordings in formats such as .wav or .mp3. These files may contain various background noises and volume inconsistencies.\n",
    "\n",
    "**Output**\n",
    "Processed Feature Matrices: The output consists of structured data that represents extracted features from the audio signals, typically including:\n",
    "Mel-Frequency Cepstral Coefficients (MFCC)\n",
    "Chroma Features\n",
    "Mel Spectrograms\n",
    "Openl3 audio features\n",
    "\n",
    "**Intermediate Data Structure**\n",
    "Features DataFrame: A 2D structure where each row represents a different audio sample, and each column represents a specific feature extracted from that sample. This DataFrame is usually in a format such as a Pandas DataFrame in Python, facilitating easy manipulation and access.\n",
    "\n",
    "**Description**\n",
    "In the transformation stage, the following steps are performed:\n",
    "\n",
    "Noise Reduction:\n",
    "Background noise is minimized using algorithms that enhance audio quality.\n",
    "\n",
    "Channel Conversion:\n",
    "Stereo audio recordings are converted into mono format to standardize the input data.\n",
    "\n",
    "Volume Normalization:\n",
    "Adjusting the amplitude of the audio signals to ensure consistent volume levels across different recordings.\n",
    "\n",
    "Feature Extraction:\n",
    "Extracting relevant features such as MFCC and Chroma features from the processed audio signals. This information is crucial for the model to learn patterns in the audio data.\n",
    "These transformations prepare the audio data for the model stage, ensuring that it is clean and structured for effective learning.\n",
    "\n",
    "### 4.2 Model Stage\n",
    "**Input**\n",
    "Processed Feature Matrices: The output from the transformation stage serves as the input for the model stage.\n",
    "\n",
    "**Output**\n",
    "Model Predictions: The output consists of predictions made by the machine learning model, indicating the class (e.g., \"Deceptive Story\" or \"Non-Deceptive Story\") for each audio sample.\n",
    "\n",
    "**Intermediate Data Structure**\n",
    "Predictions Array: A 1D array where each entry corresponds to the predicted class for the respective audio sample, often represented as numerical labels or one-hot encoded vectors.\n",
    "\n",
    "**Description**\n",
    "In the model stage, the following processes occur:\n",
    "\n",
    "Model Selection:\n",
    "Choosing an appropriate machine learning model (e.g., Support Vector Machine, Random Forest, or Neural Network) based on the data characteristics and classification requirements.\n",
    "\n",
    "Model Training:\n",
    "The selected model is trained using the features extracted from the audio samples alongside the corresponding labels (target classes).\n",
    "\n",
    "Prediction:\n",
    "After training, the model is used to make predictions on new instances based on their feature matrices.\n",
    "This stage is vital for teaching the model to recognize patterns in the audio data that correspond to different classes.\n",
    "\n",
    "### 4.3 Ensemble Stage\n",
    "**Input**\n",
    "Model Predictions: The output from the model stage serves as the input for the ensemble stage.\n",
    "\n",
    "**Output**\n",
    "Final Classification Result: The output is the final predicted class for each audio sample, typically determined by a majority vote or averaging of predictions from multiple models.\n",
    "\n",
    "**Intermediate Data Structure**\n",
    "Ensemble Predictions Matrix: A 2D array where each row corresponds to a different audio sample and each column represents predictions from different models. This format facilitates the aggregation of predictions from various models.\n",
    "\n",
    "**Description**\n",
    "In the ensemble stage, the following processes are involved:\n",
    "\n",
    "Model Aggregation:\n",
    "Combine predictions from multiple trained models to improve reliability. This could involve techniques such as voting (majority or weighted) or averaging probabilities.\n",
    "\n",
    "Final Decision Making:\n",
    "Determine the final predicted class for each audio sample based on the aggregated results from the ensemble of models.\n",
    "\n",
    "Evaluation of Ensemble Performance:\n",
    "Assess the accuracy and other performance metrics of the ensemble predictions compared to individual model performances.\n",
    "The ensemble stage enhances the overall performance and robustness of the classification task by combining the strengths of multiple models, thereby reducing the likelihood of overfitting and improving generalization to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1nDXnzYLLH6"
   },
   "source": [
    "## 4.1 Transformation stage\n",
    "\n",
    "Describe any transformations, such as feature extraction. Identify input and output. Explain why you have chosen this transformation stage.\n",
    "\n",
    "In the audio classification prediction pipeline, the Transformation stage is crucial for processing raw audio data and extracting relevant features, providing high-quality input for the subsequent modeling stage. This stage is critical as it significantly influences the model's performance and accuracy.\n",
    "\n",
    "Input\n",
    "Raw Audio Files: The input consists of audio recordings in formats such as .wav or .mp3. These audio files may contain background noise and inconsistencies in volume.\n",
    "Output\n",
    "Processed Feature Matrix: The output data consists of various features extracted from the audio signals, typically including:\n",
    "Mel-Frequency Cepstral Coefficients (MFCC)\n",
    "Chroma Features\n",
    "Mel Spectrograms\n",
    "OpenL3 Features\n",
    "\n",
    "### Reasons for Choosing These Feature Extraction Methods\n",
    "Mel-Frequency Cepstral Coefficients (MFCC):\n",
    "Reason: MFCCs are one of the most useful features in audio signal processing, effectively capturing the spectral variations of audio, reflecting human auditory perception characteristics. By applying a mel-frequency transformation and cepstral analysis, MFCCs reduce unnecessary information from audio signals, focusing on the features that best describe the audio.\n",
    "Implementation Code: In the code, MFCC features can be extracted using the librosa.feature.mfcc method.\n",
    "\n",
    "Chroma Features:\n",
    "Reason: Chroma features are critical in music analysis as they capture the harmonic relationship between different pitches in an audio signal, independent of volume. These features are particularly well-suited for audio classification tasks, as they help the model identify the tonal structure of the audio.\n",
    "Implementation Code: Chroma features can be extracted in the code using librosa.feature.chroma_stft.\n",
    "\n",
    "Mel Spectrograms:\n",
    "Reason: Mel spectrograms provide a visual representation of audio signals by mapping the frequency axis to the mel scale, which corresponds more closely to human auditory perception. This approach effectively captures the audio's temporal and spectral characteristics, facilitating easier model training.\n",
    "Implementation Code: The mel spectrogram can be generated in the code using librosa.feature.melspectrogram.\n",
    "\n",
    "OpenL3 Features:\n",
    "Reason: OpenL3 is a deep-learning-based feature extraction method that captures more complex audio characteristics. It effectively processes various types of audio data, generating high-dimensional feature representations that enrich the model's performance. OpenL3 features are particularly suitable for music and audio classification tasks, providing rich contextual information.\n",
    "Implementation Code: In the code, features can be extracted using the OpenL3 API.\n",
    "\n",
    "### Detailed Description\n",
    "In the Transformation stage, the following steps are executed:\n",
    "\n",
    "Noise Reduction:\n",
    "The audio signal is processed using noise cancellation techniques such as spectral subtraction or Wiener filtering to reduce external interference. This process significantly improves the accuracy of feature extraction and ensures that the model learns more representative features.\n",
    "\n",
    "Channel Conversion:\n",
    "Converts stereo signals to mono signals to eliminate differences between channels. This process ensures consistency of the input data and provides a simplified signal structure for feature extraction, simplifying subsequent processing.\n",
    "\n",
    "Volume normalisation:\n",
    "Recordings with different volume levels are normalised to ensure that they are analysed at the same volume level. This normalisation step removes the bias of volume on feature extraction and ensures that model learning is not affected by volume differences.\n",
    "\n",
    "Feature Extraction:\n",
    "A combination of the above mentioned feature extraction methods is used to generate a matrix including MFCC, chroma features, Mel spectrum and OpenL3 features. Eventually these features will form a unified feature matrix for use in subsequent model training and classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0F5_kI95LuZ2"
   },
   "source": [
    "## 4.2 Model Stage\n",
    "\n",
    "Describe the ML model(s) that you will build. Explain why you have chosen them.\n",
    "\n",
    "In the audio classification prediction pipeline, the Model Stage is a critical component. The primary goal of this stage is to construct and train machine learning models capable of efficiently recognizing and classifying audio data. This section will detail the chosen models, analyze their pros and cons, and explain the reasons for selecting them.\n",
    "\n",
    "Chosen Models\n",
    "\n",
    "### Deep Neural Network (DNN):\n",
    "Description: A deep neural network is a classic machine learning model that can learn complex feature representations through multiple layers of neurons. In the context of audio classification, the DNN can effectively process features extracted from audio signals (such as MFCC, chroma features, mel spectrograms, and OpenL3 features) and combine them through nonlinear activation functions for classification.\n",
    "Advantages:\n",
    "Simple structure, easy to implement, suitable for rapid development and prototyping.\n",
    "Performs well when dealing with high-dimensional feature inputs and has good learning capability.\n",
    "Disadvantages:\n",
    "Can be sensitive to the selection and processing of features, requiring optimization for specific tasks.\n",
    "As the number of layers increases, the model may face the issue of overfitting.\n",
    "\n",
    "### Transformer Model:\n",
    "Description: The Transformer model is a modern deep learning architecture that has shown exceptional performance in handling sequential data. It can capture long-range dependencies within input data through self-attention mechanisms, making it particularly suitable for context learning in audio classification tasks.\n",
    "Advantages:\n",
    "Capable of handling arbitrarily long input sequences, excelling in capturing long-term dependencies.\n",
    "Allows for parallel processing, resulting in faster training, making it suitable for large datasets.\n",
    "Disadvantages:\n",
    "Compared to traditional models, Transformers may have higher computational and memory requirements, necessitating more resources for training and tuning.\n",
    "The architecture is complex, and the tuning process may be more challenging.\n",
    "### Reasons for Choosing These Models\n",
    "Performance and Accuracy: Deep learning models (like DNNs and Transformers) can effectively learn from complex audio features, leading to higher classification performance in audio classification tasks. These models have demonstrated strong performance across various tasks, making them ideal choices.\n",
    "\n",
    "Diversity and Adaptability: The combination of DNNs and Transformer models allows for the utilization of their respective strengths. DNNs are well-suited for simple classification tasks after feature extraction, while Transformers can handle contextual relationships, performing well with more complex audio data. This diversity enables the models to adapt to various audio classification tasks, such as emotion analysis and music genre recognition.\n",
    "\n",
    "Scalability and Flexibility: The choice of deep learning models also considers future scalability and updates. Once new audio features or datasets are available, these models can easily be adjusted and improved to maintain competitiveness. Additionally, leveraging widely-used deep learning frameworks (such as PyTorch) facilitates model training and optimization.\n",
    "\n",
    "Successful Cases: DNNs and Transformer models have been proven effective in numerous research studies and applications related to audio classification, achieving good results in handling complex audio features. The selection of these models is based on successful experiences and reliable performance in practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Ensemble stage\n",
    "\n",
    "Describe any ensemble approach you might have included. Explain why you have chosen them.\n",
    "\n",
    "Using OpenL3, Transformer, and DNN together in an audio classification task provides a well-rounded approach that leverages the strengths of each model type, resulting in improved overall performance. Here are the detailed benefits and reasons for choosing this combination:\n",
    "\n",
    "### OpenL3 Advantages\n",
    "Rich Audio Feature Extraction: OpenL3, built on deep learning models like convolutional neural networks, extracts high-level, multi-layered features from audio signals. These features capture subtle audio variations, which are crucial for tasks like emotion recognition and sound source classification.\n",
    "\n",
    "Convenience of Pre-trained Models: OpenL3 offers pre-trained models, allowing for rapid deployment with minimal computational resources and time, making it highly effective for the feature extraction stage.\n",
    "\n",
    "### Transformer Advantages\n",
    "Long-term Dependency Capture: The Transformer model’s self-attention mechanism is excellent for capturing long-duration sequence information, which is ideal for tasks focusing on global structural features of audio, such as melody recognition and lengthy audio segment classification.\n",
    "\n",
    "Attention Mechanism: Transformers selectively focus on relevant parts of the input feature based on their significance, enhancing the model's predictive accuracy by emphasizing crucial aspects of the audio.\n",
    "\n",
    "### DNN Advantages\n",
    "Flexible Architecture: DNNs are highly adaptable and can be configured to suit specific tasks or data types. This architectural flexibility allows seamless integration with features extracted by OpenL3 and outputs from Transformers.\n",
    "\n",
    "Robust General Performance: DNNs perform complex pattern fitting through deep layers of non-linear transformations, making them standard choices for classification tasks.\n",
    "\n",
    "### Combined Advantages\n",
    "Multimodal Feature Integration: Using OpenL3 to provide a foundation of enriched audio features, Transformers to amplify both short and long-term information extraction, and DNNs to add complexity and adaptability at the classification stage results in comprehensive feature capturing.\n",
    "\n",
    "Enhanced Robustness and Generalization: Each model captures different aspects of audio information; combining OpenL3, Transformer, and DNN reduces the risk of single-model failure in specific scenarios and improves prediction on unseen data.\n",
    "\n",
    "Adaptability to Diverse Task Requirements: Beyond improved classification accuracy, this combination can cater to various audio analysis tasks, from signal noise reduction to complex emotion recognition.\n",
    "\n",
    "Efficient Development Process: By leveraging pre-extracted features from OpenL3 and the adaptable configuration of Transformers and DNNs, the model tuning and training processes require minimal parameter adjustments to achieve optimal results, thus accelerating the development cycle.\n",
    "\n",
    "Handling Complex Audio Environments: In noisy backgrounds or situations with multiple overlapping sounds, multi-model integration can better distinguish and classify audio signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZQPxztuL9AW"
   },
   "source": [
    "# 5 Dataset\n",
    "\n",
    "Describe the datasets that you will create to build and evaluate your models. Your datasets need to be based on our MLEnd Deception Dataset. After describing the datasets, build them here. You can explore and visualise the datasets here as well. \n",
    "\n",
    "If you are building separate training and validatio datasets, do it here. Explain clearly how you are building such datasets, how you are ensuring that they serve their purpose (i.e. they are independent and consist of IID samples) and any limitations you might think of. It is always important to identify any limitations as early as possible. The scope and validity of your conclusions will depend on your ability to understand the limitations of your approach.\n",
    "\n",
    "If you are exploring different datasets, create different subsections for each dataset and give them a name (e.g. 5.1 Dataset A, 5.2 Dataset B, 5.3 Dataset 5.3) .\n",
    "\n",
    "In developing and evaluating models based on the MLEnd Deception Dataset, we will create two main datasets: the training dataset and the test dataset. These datasets will be used for training deep learning models aimed at classifying story types (true stories vs. deceptive stories).\n",
    "\n",
    "### 5.1 Dataset Construction\n",
    "**5.1.1 Training Dataset**\n",
    "Construction Method: We will read audio data from the original MLEnd Deception Dataset using a CSV file, process each audio file (including loudness normalization, noise reduction, and feature extraction), and use approximately 80% of the samples as the training dataset.\n",
    "Feature Extraction: For each audio file, we will extract MFCC, Chroma, Mel spectrogram, and OpenL3 features, resulting in each sample having 692 dimensions (40 MFCC + 12 Chroma + 128 Mel + 512 OpenL3).\n",
    "Labeling: We will convert story types to numerical labels using a label dictionary (0 for \"True Story\" and 1 for \"Deceptive Story\").\n",
    "**5.1.2 Test Dataset**\n",
    "Construction Method: The test dataset will consist of approximately 20% of the original samples that are randomly selected, ensuring that there is no overlap with the training dataset. Similar preprocessing and feature extraction will be applied to the test dataset.\n",
    "Purpose: The test dataset will help evaluate the model's generalization capabilities and performance on unseen data.\n",
    "**5.1.3 Independence and IID Assurance**\n",
    "Independence: By using random sampling methods to ensure that the training dataset and test dataset do not overlap, we maintain the independence of the datasets.\n",
    "IID Distribution: Assuming that the original dataset samples are drawn from the same distribution, random splitting helps ensure that each subset contains samples that are independent and identically distributed (IID).\n",
    "### 5.2 Dataset Limitations\n",
    "Sampling Bias: If the original dataset contains class imbalance or inherent biases, this could affect the composition of both the training and test datasets, necessitating balance during preprocessing.\n",
    "Limited Quantity: An insufficient number of samples can lead to overfitting of the model. Therefore, careful adjustment of hyperparameters and the use of regularization techniques will be important.\n",
    "Non-IID Assumption: While we assume that the data meets the IID condition, there might be scenarios, such as with time-series data, where samples exhibit correlations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qf7GN1aeXJI"
   },
   "source": [
    "# 6 Experiments and results\n",
    "\n",
    "Carry out your experiments here. Analyse and explain your results. Unexplained results are worthless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available.\n",
      "Number of GPUs: 1\n",
      "Current GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import openl3\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "csv_path = './Deception-main/CBU0521DD_stories_attributes.csv'\n",
    "input_path = './Deception-main/CBU0521DD_stories'\n",
    "\n",
    "\n",
    "# Check for available GPUs\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available.\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"No GPU available.\")\n",
    "    \n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Read label file\n",
    "df = pd.read_csv(csv_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-25T08:58:37.245960Z",
     "start_time": "2024-12-25T08:58:37.231838500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Processing audio data\n",
    "def preprocess_audio(file_path):\n",
    "    \"\"\"\n",
    "    Read audio files, convert to mono, perform loudness normalisation and noise reduction.\n",
    "    \n",
    "    Parameters\n",
    "    file_path (str): path to the audio file.\n",
    "    \n",
    "    Returns\n",
    "    y_denoised (np.ndarray): processed audio signal.\n",
    "    \"\"\"\n",
    "    # Load audio files and convert to mono\n",
    "    y, sr = librosa.load(file_path, sr=22050, mono=True)  # Setting a fixed sample rate and ensuring mono sound\n",
    "\n",
    "    # loudness normalisation\n",
    "    y_normalized = librosa.util.normalize(y)\n",
    "    \n",
    "    # Low-pass filter for removing high-frequency noise (limiting the frequency to 3000 Hz)\n",
    "    y_lowpass = librosa.effects.preemphasis(y_normalized)\n",
    "    \n",
    "    # Spectrum acquisition using short-time Fourier transform (STFT)\n",
    "    stft = librosa.stft(y_lowpass, n_fft=2048, hop_length=512)\n",
    "    magnitude, phase = librosa.magphase(stft)\n",
    "    \n",
    "    # Noise reduction using spectral subtraction\n",
    "    noise_profile = np.mean(magnitude[:, :5], axis=1, keepdims=True)  # Counting the noise spectrum of the first 5 frames\n",
    "    magnitude_denoised = magnitude - noise_profile  # Subtracting the noise spectrum\n",
    "    magnitude_denoised = np.maximum(magnitude_denoised, 0)  # Ensure no negative values\n",
    "\n",
    "    # Restore audio signal\n",
    "    stft_denoised = magnitude_denoised * phase\n",
    "    y_denoised = librosa.istft(stft_denoised)\n",
    "\n",
    "    # Further normalisation\n",
    "    y_denoised = librosa.util.normalize(y_denoised)\n",
    "\n",
    "    return y_denoised, sr\n",
    "\n",
    "# Extracting features of pre-processed audio\n",
    "def extract_features(y, sr, model=None):\n",
    "    \"\"\"\n",
    "    Extracts audio features.\n",
    "    \n",
    "    Parameters\n",
    "    y (np.ndarray): the processed audio signal.\n",
    "    sr (int): audio sample rate.\n",
    "    model (tf.keras.Model, optional): the OpenL3 model, the model will be loaded if default is None.\n",
    "    \n",
    "    Returns\n",
    "    feature_vector (np.ndarray): A vector containing the extracted audio features.\n",
    "    \"\"\"\n",
    "    # Extraction of MFCC features\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "    mfcc_mean = np.mean(mfccs, axis=1)\n",
    "\n",
    "    # Extraction of Chroma features\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    chroma_mean = np.mean(chroma, axis=1)\n",
    "\n",
    "    # Extraction of Mel spectrogram features\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    mel_mean = np.mean(mel_spectrogram, axis=1)\n",
    "    \n",
    "    # Extracting Audio Features with OpenL3\n",
    "    if model is None:\n",
    "        # Load the music model correctly, providing all the necessary parameters\n",
    "        model = openl3.models.load_audio_embedding_model(\n",
    "            content_type='music',  \n",
    "            input_repr='mel256',\n",
    "            embedding_size=512\n",
    "        )\n",
    "\n",
    "    # Extracting OpenL3 Features\n",
    "    features, timestamps = openl3.get_audio_embedding(\n",
    "        y,\n",
    "        sr,\n",
    "        model=model,\n",
    "        input_repr='mel256',\n",
    "        embedding_size=512\n",
    "    )\n",
    "    \n",
    "    # Calculate the average of OpenL3 features\n",
    "    openl3_features = np.mean(features, axis=0)\n",
    "    \n",
    "    # Merge all extracted features\n",
    "    feature_vector = np.hstack((mfcc_mean, chroma_mean, mel_mean, openl3_features))\n",
    "\n",
    "    return feature_vector\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-25T08:58:37.296072600Z",
     "start_time": "2024-12-25T08:58:37.242838400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-25T10:29:48.399391500Z",
     "start_time": "2024-12-25T08:58:37.255055500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 43s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:   1%|          | 1/100 [00:47<1:18:23, 47.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 43s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:   2%|▏         | 2/100 [01:34<1:16:52, 47.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 61s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:   3%|▎         | 3/100 [02:40<1:29:57, 55.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 55s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:   4%|▍         | 4/100 [03:41<1:32:42, 57.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 44s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:   5%|▌         | 5/100 [04:29<1:25:57, 54.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 43s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:   6%|▌         | 6/100 [05:16<1:21:08, 51.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 53s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:   7%|▋         | 7/100 [06:14<1:23:41, 54.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 44s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:   8%|▊         | 8/100 [07:03<1:20:15, 52.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 42s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:   9%|▉         | 9/100 [07:50<1:16:34, 50.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 49s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  10%|█         | 10/100 [08:43<1:16:57, 51.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 44s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  11%|█         | 11/100 [09:31<1:14:34, 50.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 48s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  12%|█▏        | 12/100 [10:23<1:14:41, 50.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 41s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  13%|█▎        | 13/100 [11:08<1:11:13, 49.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 58s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  14%|█▍        | 14/100 [12:12<1:16:40, 53.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 56s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  15%|█▌        | 15/100 [13:13<1:19:18, 55.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 75s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  16%|█▌        | 16/100 [14:36<1:29:28, 63.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 48s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  17%|█▋        | 17/100 [15:28<1:23:45, 60.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 39s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  18%|█▊        | 18/100 [16:11<1:15:18, 55.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 47s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  19%|█▉        | 19/100 [17:02<1:12:50, 53.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 42s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  20%|██        | 20/100 [17:48<1:08:52, 51.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 47s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  21%|██        | 21/100 [18:40<1:07:56, 51.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 43s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  22%|██▏       | 22/100 [19:27<1:05:13, 50.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 42s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  23%|██▎       | 23/100 [20:13<1:02:55, 49.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 64s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  24%|██▍       | 24/100 [21:23<1:10:02, 55.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 50s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  25%|██▌       | 25/100 [22:18<1:08:50, 55.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 49s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  26%|██▌       | 26/100 [23:11<1:07:23, 54.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 54s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  27%|██▋       | 27/100 [24:10<1:07:58, 55.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 46s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  28%|██▊       | 28/100 [25:01<1:05:10, 54.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 60s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  29%|██▉       | 29/100 [26:06<1:08:12, 57.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 52s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  30%|███       | 30/100 [27:03<1:07:02, 57.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 57s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  31%|███       | 31/100 [28:05<1:07:41, 58.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 48s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  32%|███▏      | 32/100 [28:58<1:04:36, 57.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 51s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  33%|███▎      | 33/100 [29:54<1:03:13, 56.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 53s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  34%|███▍      | 34/100 [30:52<1:02:46, 57.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 48s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  35%|███▌      | 35/100 [31:44<1:00:25, 55.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 54s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  36%|███▌      | 36/100 [32:43<1:00:26, 56.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 43s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  37%|███▋      | 37/100 [33:30<56:27, 53.77s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 54s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  38%|███▊      | 38/100 [34:29<57:14, 55.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 64s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  39%|███▉      | 39/100 [35:39<1:00:38, 59.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 45s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  40%|████      | 40/100 [36:28<56:32, 56.54s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 46s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  41%|████      | 41/100 [37:19<53:57, 54.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 58s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  42%|████▏     | 42/100 [38:23<55:38, 57.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 41s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  43%|████▎     | 43/100 [39:08<51:11, 53.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 45s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  44%|████▍     | 44/100 [39:58<49:04, 52.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 45s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  45%|████▌     | 45/100 [40:48<47:22, 51.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 37s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  46%|████▌     | 46/100 [41:28<43:31, 48.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 40s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  47%|████▋     | 47/100 [42:12<41:28, 46.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 63s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  48%|████▊     | 48/100 [43:21<46:24, 53.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 54s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  49%|████▉     | 49/100 [44:20<47:01, 55.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 48s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  50%|█████     | 50/100 [45:12<45:18, 54.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 73s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  51%|█████     | 51/100 [46:32<50:34, 61.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 49s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  52%|█████▏    | 52/100 [47:25<47:28, 59.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 21s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  53%|█████▎    | 53/100 [47:48<37:59, 48.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 42s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  54%|█████▍    | 54/100 [48:35<36:40, 47.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 55s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  55%|█████▌    | 55/100 [49:35<38:35, 51.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 58s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  56%|█████▌    | 56/100 [50:38<40:24, 55.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 57s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  57%|█████▋    | 57/100 [51:40<40:58, 57.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 49s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  58%|█████▊    | 58/100 [52:34<39:16, 56.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 59s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  59%|█████▉    | 59/100 [53:38<39:59, 58.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 50s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  60%|██████    | 60/100 [54:32<38:10, 57.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 57s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  61%|██████    | 61/100 [55:35<38:15, 58.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 42s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  62%|██████▏   | 62/100 [56:21<34:46, 54.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 59s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  63%|██████▎   | 63/100 [57:25<35:38, 57.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 48s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  64%|██████▍   | 64/100 [58:17<33:40, 56.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 56s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  65%|██████▌   | 65/100 [59:19<33:40, 57.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 61s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  66%|██████▌   | 66/100 [1:00:26<34:14, 60.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 48s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  67%|██████▋   | 67/100 [1:01:18<32:00, 58.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 49s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  68%|██████▊   | 68/100 [1:02:12<30:14, 56.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 52s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  69%|██████▉   | 69/100 [1:03:08<29:18, 56.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 51s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  70%|███████   | 70/100 [1:04:05<28:17, 56.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 45s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  71%|███████   | 71/100 [1:04:55<26:22, 54.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 57s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  72%|███████▏  | 72/100 [1:05:58<26:39, 57.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 72s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  73%|███████▎  | 73/100 [1:07:16<28:35, 63.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 48s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  74%|███████▍  | 74/100 [1:08:09<26:08, 60.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 56s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  75%|███████▌  | 75/100 [1:09:11<25:19, 60.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 53s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  76%|███████▌  | 76/100 [1:10:09<23:59, 59.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 38s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  77%|███████▋  | 77/100 [1:10:51<20:55, 54.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 52s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  78%|███████▊  | 78/100 [1:11:48<20:18, 55.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 38s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  79%|███████▉  | 79/100 [1:12:30<18:00, 51.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 43s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  80%|████████  | 80/100 [1:13:18<16:46, 50.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 55s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  81%|████████  | 81/100 [1:14:18<16:52, 53.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 48s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  82%|████████▏ | 82/100 [1:15:12<15:58, 53.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 53s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  83%|████████▎ | 83/100 [1:16:09<15:28, 54.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 62s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  84%|████████▍ | 84/100 [1:17:18<15:40, 58.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 47s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  85%|████████▌ | 85/100 [1:18:09<14:08, 56.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 43s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  86%|████████▌ | 86/100 [1:18:57<12:33, 53.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 63s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  87%|████████▋ | 87/100 [1:20:06<12:40, 58.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 50s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  88%|████████▊ | 88/100 [1:21:02<11:30, 57.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 44s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  89%|████████▉ | 89/100 [1:21:50<10:02, 54.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 64s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  90%|█████████ | 90/100 [1:23:00<09:54, 59.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 39s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  91%|█████████ | 91/100 [1:23:43<08:10, 54.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 42s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  92%|█████████▏| 92/100 [1:24:29<06:54, 51.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 39s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  93%|█████████▎| 93/100 [1:25:11<05:43, 49.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 43s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  94%|█████████▍| 94/100 [1:25:59<04:52, 48.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 55s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  95%|█████████▌| 95/100 [1:26:59<04:20, 52.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 39s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  96%|█████████▌| 96/100 [1:27:42<03:17, 49.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 53s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  97%|█████████▋| 97/100 [1:28:40<02:35, 51.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 38s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  98%|█████████▊| 98/100 [1:29:22<01:38, 49.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 39s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  99%|█████████▉| 99/100 [1:30:05<00:47, 47.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 60s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data: 100%|██████████| 100/100 [1:31:10<00:00, 54.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Shape: torch.Size([100, 692])\n",
      "Label Dataset Shape: torch.Size([100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12564\\3570623171.py:28: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  train_dataset = torch.tensor(train_dataset, dtype=torch.float).to(device)\n"
     ]
    }
   ],
   "source": [
    "# Define the label dictionary\n",
    "label_dict = {\n",
    "    \"True Story\": 0,      \n",
    "    \"Deceptive Story\": 1  \n",
    "}\n",
    "\n",
    "# Constructing training data and labels\n",
    "train_dataset = []\n",
    "label_dataset = []\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing Data\"):\n",
    "    filename = row['filename']\n",
    "    story_type = row['Story_type']\n",
    "    \n",
    "    # Building audio file paths\n",
    "    file_path = os.path.join(input_path, filename)\n",
    "\n",
    "    # Read and process audio signals\n",
    "    y_denoised, sr = preprocess_audio(file_path)\n",
    "    \n",
    "    # Extracting audio features\n",
    "    features = extract_features(y_denoised, sr)\n",
    "    \n",
    "    train_dataset.append(features)\n",
    "    label_dataset.append(label_dict[story_type])  # Converting types to labels based on a dictionary\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "train_dataset = torch.tensor(train_dataset, dtype=torch.float).to(device)\n",
    "label_dataset = torch.tensor(label_dataset, dtype=torch.long).to(device)\n",
    "\n",
    "print(\"Train Dataset Shape:\", train_dataset.shape)\n",
    "print(\"Label Dataset Shape:\", label_dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train Loss: 1.0352, Train Accuracy: 0.5250\n",
      "Epoch: 10, Test Loss: 1.3503, Test Accuracy: 0.6000\n",
      "-------------\n",
      "Saved best model with Test Loss: 1.3503 and Test Accuracy: 0.6000\n",
      "Epoch: 20, Train Loss: 0.8301, Train Accuracy: 0.5000\n",
      "Epoch: 20, Test Loss: 1.0939, Test Accuracy: 0.2500\n",
      "-------------\n",
      "Saved best model with Test Loss: 1.0939 and Test Accuracy: 0.2500\n",
      "Epoch: 30, Train Loss: 0.7837, Train Accuracy: 0.5125\n",
      "Epoch: 30, Test Loss: 0.9872, Test Accuracy: 0.2500\n",
      "-------------\n",
      "Saved best model with Test Loss: 0.9872 and Test Accuracy: 0.2500\n",
      "Epoch: 40, Train Loss: 0.7249, Train Accuracy: 0.6000\n",
      "Epoch: 40, Test Loss: 0.9004, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Saved best model with Test Loss: 0.9004 and Test Accuracy: 0.3500\n",
      "Epoch: 50, Train Loss: 0.6779, Train Accuracy: 0.5375\n",
      "Epoch: 50, Test Loss: 0.8345, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Saved best model with Test Loss: 0.8345 and Test Accuracy: 0.3000\n",
      "Epoch: 60, Train Loss: 0.6432, Train Accuracy: 0.6125\n",
      "Epoch: 60, Test Loss: 0.7875, Test Accuracy: 0.5500\n",
      "-------------\n",
      "Saved best model with Test Loss: 0.7875 and Test Accuracy: 0.5500\n",
      "Epoch: 70, Train Loss: 0.6182, Train Accuracy: 0.6250\n",
      "Epoch: 70, Test Loss: 0.7550, Test Accuracy: 0.5500\n",
      "-------------\n",
      "Saved best model with Test Loss: 0.7550 and Test Accuracy: 0.5500\n",
      "Epoch: 80, Train Loss: 0.5986, Train Accuracy: 0.6125\n",
      "Epoch: 80, Test Loss: 0.7332, Test Accuracy: 0.5500\n",
      "-------------\n",
      "Saved best model with Test Loss: 0.7332 and Test Accuracy: 0.5500\n",
      "Epoch: 90, Train Loss: 0.5820, Train Accuracy: 0.6625\n",
      "Epoch: 90, Test Loss: 0.7216, Test Accuracy: 0.5500\n",
      "-------------\n",
      "Saved best model with Test Loss: 0.7216 and Test Accuracy: 0.5500\n",
      "Epoch: 100, Train Loss: 0.5675, Train Accuracy: 0.6750\n",
      "Epoch: 100, Test Loss: 0.7171, Test Accuracy: 0.5500\n",
      "-------------\n",
      "Saved best model with Test Loss: 0.7171 and Test Accuracy: 0.5500\n",
      "Epoch: 110, Train Loss: 0.5543, Train Accuracy: 0.7000\n",
      "Epoch: 110, Test Loss: 0.7185, Test Accuracy: 0.5000\n",
      "-------------\n",
      "Epoch: 120, Train Loss: 0.5420, Train Accuracy: 0.7125\n",
      "Epoch: 120, Test Loss: 0.7230, Test Accuracy: 0.5000\n",
      "-------------\n",
      "Epoch: 130, Train Loss: 0.5305, Train Accuracy: 0.7875\n",
      "Epoch: 130, Test Loss: 0.7286, Test Accuracy: 0.5000\n",
      "-------------\n",
      "Epoch: 140, Train Loss: 0.5196, Train Accuracy: 0.8125\n",
      "Epoch: 140, Test Loss: 0.7336, Test Accuracy: 0.5000\n",
      "-------------\n",
      "Epoch: 150, Train Loss: 0.5088, Train Accuracy: 0.8250\n",
      "Epoch: 150, Test Loss: 0.7367, Test Accuracy: 0.5000\n",
      "-------------\n",
      "Epoch: 160, Train Loss: 0.4985, Train Accuracy: 0.8250\n",
      "Epoch: 160, Test Loss: 0.7411, Test Accuracy: 0.5000\n",
      "-------------\n",
      "Epoch: 170, Train Loss: 0.4886, Train Accuracy: 0.8250\n",
      "Epoch: 170, Test Loss: 0.7475, Test Accuracy: 0.5000\n",
      "-------------\n",
      "Epoch: 180, Train Loss: 0.4788, Train Accuracy: 0.8250\n",
      "Epoch: 180, Test Loss: 0.7576, Test Accuracy: 0.5000\n",
      "-------------\n",
      "Epoch: 190, Train Loss: 0.4691, Train Accuracy: 0.8375\n",
      "Epoch: 190, Test Loss: 0.7642, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 200, Train Loss: 0.4600, Train Accuracy: 0.8375\n",
      "Epoch: 200, Test Loss: 0.7714, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 210, Train Loss: 0.4511, Train Accuracy: 0.8375\n",
      "Epoch: 210, Test Loss: 0.7785, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 220, Train Loss: 0.4425, Train Accuracy: 0.8500\n",
      "Epoch: 220, Test Loss: 0.7865, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 230, Train Loss: 0.4340, Train Accuracy: 0.8500\n",
      "Epoch: 230, Test Loss: 0.7945, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 240, Train Loss: 0.4258, Train Accuracy: 0.8625\n",
      "Epoch: 240, Test Loss: 0.8029, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 250, Train Loss: 0.4178, Train Accuracy: 0.8750\n",
      "Epoch: 250, Test Loss: 0.8107, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 260, Train Loss: 0.4098, Train Accuracy: 0.8875\n",
      "Epoch: 260, Test Loss: 0.8214, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 270, Train Loss: 0.4021, Train Accuracy: 0.8875\n",
      "Epoch: 270, Test Loss: 0.8314, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 280, Train Loss: 0.3945, Train Accuracy: 0.9000\n",
      "Epoch: 280, Test Loss: 0.8409, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 290, Train Loss: 0.3870, Train Accuracy: 0.9125\n",
      "Epoch: 290, Test Loss: 0.8491, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 300, Train Loss: 0.3796, Train Accuracy: 0.9125\n",
      "Epoch: 300, Test Loss: 0.8598, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 310, Train Loss: 0.3724, Train Accuracy: 0.9125\n",
      "Epoch: 310, Test Loss: 0.8698, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 320, Train Loss: 0.3653, Train Accuracy: 0.9125\n",
      "Epoch: 320, Test Loss: 0.8805, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 330, Train Loss: 0.3583, Train Accuracy: 0.9125\n",
      "Epoch: 330, Test Loss: 0.8914, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 340, Train Loss: 0.3514, Train Accuracy: 0.9250\n",
      "Epoch: 340, Test Loss: 0.9023, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 350, Train Loss: 0.3445, Train Accuracy: 0.9250\n",
      "Epoch: 350, Test Loss: 0.9144, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 360, Train Loss: 0.3378, Train Accuracy: 0.9250\n",
      "Epoch: 360, Test Loss: 0.9259, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 370, Train Loss: 0.3311, Train Accuracy: 0.9250\n",
      "Epoch: 370, Test Loss: 0.9384, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 380, Train Loss: 0.3245, Train Accuracy: 0.9250\n",
      "Epoch: 380, Test Loss: 0.9502, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 390, Train Loss: 0.3181, Train Accuracy: 0.9250\n",
      "Epoch: 390, Test Loss: 0.9613, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 400, Train Loss: 0.3117, Train Accuracy: 0.9250\n",
      "Epoch: 400, Test Loss: 0.9729, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 410, Train Loss: 0.3054, Train Accuracy: 0.9250\n",
      "Epoch: 410, Test Loss: 0.9852, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 420, Train Loss: 0.2993, Train Accuracy: 0.9250\n",
      "Epoch: 420, Test Loss: 0.9976, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 430, Train Loss: 0.2931, Train Accuracy: 0.9250\n",
      "Epoch: 430, Test Loss: 1.0079, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 440, Train Loss: 0.2870, Train Accuracy: 0.9375\n",
      "Epoch: 440, Test Loss: 1.0214, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 450, Train Loss: 0.2811, Train Accuracy: 0.9250\n",
      "Epoch: 450, Test Loss: 1.0334, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 460, Train Loss: 0.2753, Train Accuracy: 0.9250\n",
      "Epoch: 460, Test Loss: 1.0447, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 470, Train Loss: 0.2695, Train Accuracy: 0.9375\n",
      "Epoch: 470, Test Loss: 1.0580, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 480, Train Loss: 0.2638, Train Accuracy: 0.9375\n",
      "Epoch: 480, Test Loss: 1.0711, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 490, Train Loss: 0.2583, Train Accuracy: 0.9375\n",
      "Epoch: 490, Test Loss: 1.0833, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 500, Train Loss: 0.2529, Train Accuracy: 0.9375\n",
      "Epoch: 500, Test Loss: 1.0964, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 510, Train Loss: 0.2475, Train Accuracy: 0.9375\n",
      "Epoch: 510, Test Loss: 1.1091, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 520, Train Loss: 0.2423, Train Accuracy: 0.9500\n",
      "Epoch: 520, Test Loss: 1.1222, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 530, Train Loss: 0.2371, Train Accuracy: 0.9500\n",
      "Epoch: 530, Test Loss: 1.1350, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 540, Train Loss: 0.2320, Train Accuracy: 0.9625\n",
      "Epoch: 540, Test Loss: 1.1470, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 550, Train Loss: 0.2271, Train Accuracy: 0.9625\n",
      "Epoch: 550, Test Loss: 1.1603, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 560, Train Loss: 0.2224, Train Accuracy: 0.9625\n",
      "Epoch: 560, Test Loss: 1.1717, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 570, Train Loss: 0.2176, Train Accuracy: 0.9625\n",
      "Epoch: 570, Test Loss: 1.1847, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 580, Train Loss: 0.2130, Train Accuracy: 0.9750\n",
      "Epoch: 580, Test Loss: 1.1986, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 590, Train Loss: 0.2085, Train Accuracy: 0.9750\n",
      "Epoch: 590, Test Loss: 1.2111, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 600, Train Loss: 0.2041, Train Accuracy: 0.9750\n",
      "Epoch: 600, Test Loss: 1.2253, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 610, Train Loss: 0.1997, Train Accuracy: 0.9750\n",
      "Epoch: 610, Test Loss: 1.2381, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 620, Train Loss: 0.1955, Train Accuracy: 0.9750\n",
      "Epoch: 620, Test Loss: 1.2512, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 630, Train Loss: 0.1913, Train Accuracy: 0.9750\n",
      "Epoch: 630, Test Loss: 1.2641, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 640, Train Loss: 0.1872, Train Accuracy: 0.9750\n",
      "Epoch: 640, Test Loss: 1.2771, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 650, Train Loss: 0.1831, Train Accuracy: 0.9875\n",
      "Epoch: 650, Test Loss: 1.2899, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 660, Train Loss: 0.1791, Train Accuracy: 0.9875\n",
      "Epoch: 660, Test Loss: 1.3040, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 670, Train Loss: 0.1752, Train Accuracy: 0.9875\n",
      "Epoch: 670, Test Loss: 1.3173, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 680, Train Loss: 0.1713, Train Accuracy: 0.9875\n",
      "Epoch: 680, Test Loss: 1.3313, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 690, Train Loss: 0.1675, Train Accuracy: 0.9875\n",
      "Epoch: 690, Test Loss: 1.3448, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 700, Train Loss: 0.1639, Train Accuracy: 0.9875\n",
      "Epoch: 700, Test Loss: 1.3584, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 710, Train Loss: 0.1603, Train Accuracy: 0.9875\n",
      "Epoch: 710, Test Loss: 1.3719, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 720, Train Loss: 0.1568, Train Accuracy: 0.9875\n",
      "Epoch: 720, Test Loss: 1.3864, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 730, Train Loss: 0.1534, Train Accuracy: 0.9875\n",
      "Epoch: 730, Test Loss: 1.3997, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 740, Train Loss: 0.1501, Train Accuracy: 0.9875\n",
      "Epoch: 740, Test Loss: 1.4126, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 750, Train Loss: 0.1469, Train Accuracy: 0.9875\n",
      "Epoch: 750, Test Loss: 1.4272, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 760, Train Loss: 0.1437, Train Accuracy: 0.9875\n",
      "Epoch: 760, Test Loss: 1.4432, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 770, Train Loss: 0.1406, Train Accuracy: 0.9875\n",
      "Epoch: 770, Test Loss: 1.4538, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 780, Train Loss: 0.1376, Train Accuracy: 0.9875\n",
      "Epoch: 780, Test Loss: 1.4681, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 790, Train Loss: 0.1347, Train Accuracy: 0.9875\n",
      "Epoch: 790, Test Loss: 1.4833, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 800, Train Loss: 0.1318, Train Accuracy: 0.9875\n",
      "Epoch: 800, Test Loss: 1.4975, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 810, Train Loss: 0.1290, Train Accuracy: 0.9875\n",
      "Epoch: 810, Test Loss: 1.5094, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 820, Train Loss: 0.1263, Train Accuracy: 0.9875\n",
      "Epoch: 820, Test Loss: 1.5240, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 830, Train Loss: 0.1236, Train Accuracy: 0.9875\n",
      "Epoch: 830, Test Loss: 1.5384, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 840, Train Loss: 0.1209, Train Accuracy: 0.9875\n",
      "Epoch: 840, Test Loss: 1.5511, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 850, Train Loss: 0.1184, Train Accuracy: 0.9875\n",
      "Epoch: 850, Test Loss: 1.5672, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 860, Train Loss: 0.1159, Train Accuracy: 0.9875\n",
      "Epoch: 860, Test Loss: 1.5786, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 870, Train Loss: 0.1136, Train Accuracy: 0.9875\n",
      "Epoch: 870, Test Loss: 1.5931, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 880, Train Loss: 0.1111, Train Accuracy: 0.9875\n",
      "Epoch: 880, Test Loss: 1.6061, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 890, Train Loss: 0.1088, Train Accuracy: 0.9875\n",
      "Epoch: 890, Test Loss: 1.6199, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 900, Train Loss: 0.1065, Train Accuracy: 0.9875\n",
      "Epoch: 900, Test Loss: 1.6343, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 910, Train Loss: 0.1042, Train Accuracy: 1.0000\n",
      "Epoch: 910, Test Loss: 1.6488, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 920, Train Loss: 0.1020, Train Accuracy: 1.0000\n",
      "Epoch: 920, Test Loss: 1.6605, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 930, Train Loss: 0.0998, Train Accuracy: 1.0000\n",
      "Epoch: 930, Test Loss: 1.6744, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 940, Train Loss: 0.0979, Train Accuracy: 1.0000\n",
      "Epoch: 940, Test Loss: 1.6859, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 950, Train Loss: 0.0957, Train Accuracy: 1.0000\n",
      "Epoch: 950, Test Loss: 1.6996, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 960, Train Loss: 0.0937, Train Accuracy: 1.0000\n",
      "Epoch: 960, Test Loss: 1.7131, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 970, Train Loss: 0.0918, Train Accuracy: 1.0000\n",
      "Epoch: 970, Test Loss: 1.7262, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 980, Train Loss: 0.0899, Train Accuracy: 1.0000\n",
      "Epoch: 980, Test Loss: 1.7406, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 990, Train Loss: 0.0881, Train Accuracy: 1.0000\n",
      "Epoch: 990, Test Loss: 1.7524, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 1000, Train Loss: 0.0862, Train Accuracy: 1.0000\n",
      "Epoch: 1000, Test Loss: 1.7645, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Training completed. Best Test Loss: 0.7171, Best Test Accuracy: 0.5500.\n"
     ]
    }
   ],
   "source": [
    "# Defining the DNN model\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, input_size=692, hidden_size=128, output_size=2):  # 692 is the number of features = 40(MFCC) + 12(Chroma) +                                                                                      128(Mel) + 512(openl3)\n",
    "        super(DNN, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.output = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Prepare data\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_dataset, label_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialising the model and optimiser\n",
    "model = DNN().to(device) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1E-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training Models\n",
    "num_epochs = 1000  # Setting the total number of rounds of training\n",
    "best_test_loss = float('inf')  # Initialising the optimal test loss\n",
    "best_test_accuracy = 0.0  # Initialising the best test accuracy\n",
    "best_model_path = 'best_DNN_model.pth'  # Path to save the best model\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # training phase\n",
    "    model.train()  # Setting the model to training mode\n",
    "    optimizer.zero_grad()  # Zeroing the gradient\n",
    "    outputs = model(X_train)  # forward propagation\n",
    "    loss = criterion(outputs, y_train)  # Calculation of losses\n",
    "    loss.backward()  # backward propagation\n",
    "    optimizer.step()  # Updating parameters\n",
    "\n",
    "    # Checked every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        # Calculate training accuracy\n",
    "        accuracy = (outputs.argmax(1) == y_train).type(torch.float32).sum().item() / X_train.shape[0]\n",
    "        print(f\"Epoch: {epoch + 1}, Train Loss: {loss.item():.4f}, Train Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # test model\n",
    "    if (epoch + 1) % 10 == 0:  # Tested every 10 epochs\n",
    "        model.eval()  # Setting the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test)  # Forward propagation on the test set\n",
    "            test_loss = criterion(test_outputs, y_test)  # Calculating test losses\n",
    "            test_accuracy = (test_outputs.argmax(1) == y_test).type(torch.float32).sum().item() / X_test.shape[0]  # Calculating Test Accuracy\n",
    "\n",
    "            # Output test results\n",
    "            print(f\"Epoch: {epoch + 1}, Test Loss: {test_loss.item():.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "            print(\"-------------\")\n",
    "\n",
    "            # Preservation of optimal models\n",
    "            if test_loss < best_test_loss:\n",
    "                best_test_loss = test_loss\n",
    "                best_test_accuracy = test_accuracy  # Preservation of optimal accuracy\n",
    "                torch.save(model.state_dict(), best_model_path)  # Save the current best model\n",
    "                print(f\"Saved best model with Test Loss: {best_test_loss:.4f} and Test Accuracy: {best_test_accuracy:.4f}\")\n",
    "\n",
    "# Output optimal loss and accuracy at the end of training\n",
    "print(f\"Training completed. Best Test Loss: {best_test_loss:.4f}, Best Test Accuracy: {best_test_accuracy:.4f}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-25T10:39:28.273434600Z",
     "start_time": "2024-12-25T10:39:27.082218100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Opencv\\Anaconda3\\envs\\Learning\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train Loss: 0.8932, Train Accuracy: 0.5125\n",
      "Epoch: 10, Test Loss: 0.8848, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Saved best model with Test Loss: 0.8848 and Test Accuracy: 0.4500\n",
      "Epoch: 20, Train Loss: 0.6978, Train Accuracy: 0.5125\n",
      "Epoch: 20, Test Loss: 0.7670, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Saved best model with Test Loss: 0.7670 and Test Accuracy: 0.4500\n",
      "Epoch: 30, Train Loss: 0.7005, Train Accuracy: 0.4875\n",
      "Epoch: 30, Test Loss: 0.6897, Test Accuracy: 0.5500\n",
      "-------------\n",
      "Saved best model with Test Loss: 0.6897 and Test Accuracy: 0.5500\n",
      "Epoch: 40, Train Loss: 0.6947, Train Accuracy: 0.5000\n",
      "Epoch: 40, Test Loss: 0.6886, Test Accuracy: 0.5500\n",
      "-------------\n",
      "Saved best model with Test Loss: 0.6886 and Test Accuracy: 0.5500\n",
      "Epoch: 50, Train Loss: 0.6915, Train Accuracy: 0.5250\n",
      "Epoch: 50, Test Loss: 0.6973, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 60, Train Loss: 0.6849, Train Accuracy: 0.5750\n",
      "Epoch: 60, Test Loss: 0.7008, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 70, Train Loss: 0.6968, Train Accuracy: 0.4750\n",
      "Epoch: 70, Test Loss: 0.6989, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 80, Train Loss: 0.6983, Train Accuracy: 0.5000\n",
      "Epoch: 80, Test Loss: 0.6947, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 90, Train Loss: 0.6897, Train Accuracy: 0.4625\n",
      "Epoch: 90, Test Loss: 0.6931, Test Accuracy: 0.5000\n",
      "-------------\n",
      "Epoch: 100, Train Loss: 0.6851, Train Accuracy: 0.5750\n",
      "Epoch: 100, Test Loss: 0.6931, Test Accuracy: 0.5500\n",
      "-------------\n",
      "Epoch: 110, Train Loss: 0.6942, Train Accuracy: 0.5375\n",
      "Epoch: 110, Test Loss: 0.6914, Test Accuracy: 0.5000\n",
      "-------------\n",
      "Epoch: 120, Train Loss: 0.6638, Train Accuracy: 0.6125\n",
      "Epoch: 120, Test Loss: 0.6871, Test Accuracy: 0.6500\n",
      "-------------\n",
      "Saved best model with Test Loss: 0.6871 and Test Accuracy: 0.6500\n",
      "Epoch: 130, Train Loss: 0.6704, Train Accuracy: 0.5875\n",
      "Epoch: 130, Test Loss: 0.6742, Test Accuracy: 0.7000\n",
      "-------------\n",
      "Saved best model with Test Loss: 0.6742 and Test Accuracy: 0.7000\n",
      "Epoch: 140, Train Loss: 0.6086, Train Accuracy: 0.6375\n",
      "Epoch: 140, Test Loss: 0.6657, Test Accuracy: 0.5500\n",
      "-------------\n",
      "Saved best model with Test Loss: 0.6657 and Test Accuracy: 0.5500\n",
      "Epoch: 150, Train Loss: 0.5661, Train Accuracy: 0.6625\n",
      "Epoch: 150, Test Loss: 0.7121, Test Accuracy: 0.6500\n",
      "-------------\n",
      "Epoch: 160, Train Loss: 0.5550, Train Accuracy: 0.7125\n",
      "Epoch: 160, Test Loss: 0.8447, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 170, Train Loss: 0.5319, Train Accuracy: 0.7250\n",
      "Epoch: 170, Test Loss: 0.9466, Test Accuracy: 0.5000\n",
      "-------------\n",
      "Epoch: 180, Train Loss: 0.4261, Train Accuracy: 0.8000\n",
      "Epoch: 180, Test Loss: 0.9510, Test Accuracy: 0.5000\n",
      "-------------\n",
      "Epoch: 190, Train Loss: 0.3687, Train Accuracy: 0.8500\n",
      "Epoch: 190, Test Loss: 1.2230, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 200, Train Loss: 0.4301, Train Accuracy: 0.8375\n",
      "Epoch: 200, Test Loss: 1.4165, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 210, Train Loss: 0.2161, Train Accuracy: 0.9375\n",
      "Epoch: 210, Test Loss: 1.7519, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 220, Train Loss: 0.3672, Train Accuracy: 0.8250\n",
      "Epoch: 220, Test Loss: 1.3951, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 230, Train Loss: 0.2833, Train Accuracy: 0.9000\n",
      "Epoch: 230, Test Loss: 2.2268, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 240, Train Loss: 0.1551, Train Accuracy: 0.9250\n",
      "Epoch: 240, Test Loss: 2.4326, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 250, Train Loss: 0.1328, Train Accuracy: 0.9375\n",
      "Epoch: 250, Test Loss: 2.5287, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 260, Train Loss: 0.0941, Train Accuracy: 0.9875\n",
      "Epoch: 260, Test Loss: 2.8315, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 270, Train Loss: 0.1156, Train Accuracy: 0.9500\n",
      "Epoch: 270, Test Loss: 3.3720, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 280, Train Loss: 0.0978, Train Accuracy: 0.9625\n",
      "Epoch: 280, Test Loss: 3.7658, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 290, Train Loss: 0.0407, Train Accuracy: 1.0000\n",
      "Epoch: 290, Test Loss: 3.4515, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 300, Train Loss: 0.1121, Train Accuracy: 0.9250\n",
      "Epoch: 300, Test Loss: 3.9987, Test Accuracy: 0.5000\n",
      "-------------\n",
      "Epoch: 310, Train Loss: 0.1305, Train Accuracy: 0.9625\n",
      "Epoch: 310, Test Loss: 4.5302, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 320, Train Loss: 0.0440, Train Accuracy: 0.9875\n",
      "Epoch: 320, Test Loss: 3.9305, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 330, Train Loss: 0.0331, Train Accuracy: 0.9875\n",
      "Epoch: 330, Test Loss: 6.3872, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 340, Train Loss: 0.0828, Train Accuracy: 0.9750\n",
      "Epoch: 340, Test Loss: 3.1375, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 350, Train Loss: 0.0081, Train Accuracy: 1.0000\n",
      "Epoch: 350, Test Loss: 5.1152, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 360, Train Loss: 0.0021, Train Accuracy: 1.0000\n",
      "Epoch: 360, Test Loss: 5.7775, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 370, Train Loss: 0.0601, Train Accuracy: 0.9875\n",
      "Epoch: 370, Test Loss: 4.4952, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 380, Train Loss: 0.0044, Train Accuracy: 1.0000\n",
      "Epoch: 380, Test Loss: 4.3855, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 390, Train Loss: 0.0016, Train Accuracy: 1.0000\n",
      "Epoch: 390, Test Loss: 5.8396, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 400, Train Loss: 0.0050, Train Accuracy: 1.0000\n",
      "Epoch: 400, Test Loss: 4.5939, Test Accuracy: 0.5000\n",
      "-------------\n",
      "Epoch: 410, Train Loss: 0.0068, Train Accuracy: 1.0000\n",
      "Epoch: 410, Test Loss: 4.8605, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 420, Train Loss: 0.0100, Train Accuracy: 1.0000\n",
      "Epoch: 420, Test Loss: 5.0090, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 430, Train Loss: 0.0087, Train Accuracy: 1.0000\n",
      "Epoch: 430, Test Loss: 5.3758, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 440, Train Loss: 0.0046, Train Accuracy: 1.0000\n",
      "Epoch: 440, Test Loss: 5.6795, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 450, Train Loss: 0.0205, Train Accuracy: 0.9875\n",
      "Epoch: 450, Test Loss: 6.3511, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 460, Train Loss: 0.0034, Train Accuracy: 1.0000\n",
      "Epoch: 460, Test Loss: 5.7737, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 470, Train Loss: 0.0224, Train Accuracy: 0.9875\n",
      "Epoch: 470, Test Loss: 6.1072, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 480, Train Loss: 0.0173, Train Accuracy: 0.9875\n",
      "Epoch: 480, Test Loss: 6.6724, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 490, Train Loss: 0.0011, Train Accuracy: 1.0000\n",
      "Epoch: 490, Test Loss: 5.8218, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 500, Train Loss: 0.0025, Train Accuracy: 1.0000\n",
      "Epoch: 500, Test Loss: 6.8092, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 510, Train Loss: 0.0133, Train Accuracy: 0.9875\n",
      "Epoch: 510, Test Loss: 6.6328, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 520, Train Loss: 0.0010, Train Accuracy: 1.0000\n",
      "Epoch: 520, Test Loss: 7.0693, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 530, Train Loss: 0.0011, Train Accuracy: 1.0000\n",
      "Epoch: 530, Test Loss: 6.8620, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 540, Train Loss: 0.0745, Train Accuracy: 0.9875\n",
      "Epoch: 540, Test Loss: 7.4025, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 550, Train Loss: 0.0170, Train Accuracy: 0.9875\n",
      "Epoch: 550, Test Loss: 6.4259, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 560, Train Loss: 0.0462, Train Accuracy: 0.9875\n",
      "Epoch: 560, Test Loss: 5.2549, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 570, Train Loss: 0.0484, Train Accuracy: 0.9875\n",
      "Epoch: 570, Test Loss: 6.1728, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 580, Train Loss: 0.0437, Train Accuracy: 0.9750\n",
      "Epoch: 580, Test Loss: 5.0849, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 590, Train Loss: 0.0006, Train Accuracy: 1.0000\n",
      "Epoch: 590, Test Loss: 6.5908, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 600, Train Loss: 0.0009, Train Accuracy: 1.0000\n",
      "Epoch: 600, Test Loss: 6.3053, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 610, Train Loss: 0.0085, Train Accuracy: 1.0000\n",
      "Epoch: 610, Test Loss: 6.6928, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 620, Train Loss: 0.0009, Train Accuracy: 1.0000\n",
      "Epoch: 620, Test Loss: 6.3594, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 630, Train Loss: 0.0011, Train Accuracy: 1.0000\n",
      "Epoch: 630, Test Loss: 7.5347, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 640, Train Loss: 0.0036, Train Accuracy: 1.0000\n",
      "Epoch: 640, Test Loss: 6.8767, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 650, Train Loss: 0.0127, Train Accuracy: 0.9875\n",
      "Epoch: 650, Test Loss: 6.7905, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 660, Train Loss: 0.0055, Train Accuracy: 1.0000\n",
      "Epoch: 660, Test Loss: 6.5101, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 670, Train Loss: 0.0004, Train Accuracy: 1.0000\n",
      "Epoch: 670, Test Loss: 6.8569, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 680, Train Loss: 0.0220, Train Accuracy: 0.9875\n",
      "Epoch: 680, Test Loss: 7.1218, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 690, Train Loss: 0.0504, Train Accuracy: 0.9750\n",
      "Epoch: 690, Test Loss: 5.1437, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 700, Train Loss: 0.0070, Train Accuracy: 1.0000\n",
      "Epoch: 700, Test Loss: 6.2205, Test Accuracy: 0.3000\n",
      "-------------\n",
      "Epoch: 710, Train Loss: 0.0007, Train Accuracy: 1.0000\n",
      "Epoch: 710, Test Loss: 6.6367, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 720, Train Loss: 0.0002, Train Accuracy: 1.0000\n",
      "Epoch: 720, Test Loss: 7.9995, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 730, Train Loss: 0.0413, Train Accuracy: 0.9875\n",
      "Epoch: 730, Test Loss: 7.0068, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 740, Train Loss: 0.0024, Train Accuracy: 1.0000\n",
      "Epoch: 740, Test Loss: 6.9535, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 750, Train Loss: 0.0189, Train Accuracy: 0.9875\n",
      "Epoch: 750, Test Loss: 6.7131, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 760, Train Loss: 0.0044, Train Accuracy: 1.0000\n",
      "Epoch: 760, Test Loss: 6.0940, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 770, Train Loss: 0.0012, Train Accuracy: 1.0000\n",
      "Epoch: 770, Test Loss: 7.2164, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 780, Train Loss: 0.0010, Train Accuracy: 1.0000\n",
      "Epoch: 780, Test Loss: 6.3281, Test Accuracy: 0.5000\n",
      "-------------\n",
      "Epoch: 790, Train Loss: 0.0005, Train Accuracy: 1.0000\n",
      "Epoch: 790, Test Loss: 6.8526, Test Accuracy: 0.3500\n",
      "-------------\n",
      "Epoch: 800, Train Loss: 0.0159, Train Accuracy: 0.9875\n",
      "Epoch: 800, Test Loss: 6.3466, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 810, Train Loss: 0.0003, Train Accuracy: 1.0000\n",
      "Epoch: 810, Test Loss: 6.2737, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 820, Train Loss: 0.0001, Train Accuracy: 1.0000\n",
      "Epoch: 820, Test Loss: 6.4388, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 830, Train Loss: 0.0011, Train Accuracy: 1.0000\n",
      "Epoch: 830, Test Loss: 6.5123, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 840, Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch: 840, Test Loss: 6.7377, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 850, Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch: 850, Test Loss: 6.8953, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 860, Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch: 860, Test Loss: 6.9619, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 870, Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch: 870, Test Loss: 6.9844, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 880, Train Loss: 0.0001, Train Accuracy: 1.0000\n",
      "Epoch: 880, Test Loss: 6.9962, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 890, Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch: 890, Test Loss: 7.0163, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 900, Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch: 900, Test Loss: 7.0598, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 910, Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch: 910, Test Loss: 7.1344, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 920, Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch: 920, Test Loss: 7.2002, Test Accuracy: 0.4000\n",
      "-------------\n",
      "Epoch: 930, Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch: 930, Test Loss: 7.2077, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 940, Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch: 940, Test Loss: 7.2273, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 950, Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch: 950, Test Loss: 7.2393, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 960, Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch: 960, Test Loss: 7.2478, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 970, Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch: 970, Test Loss: 7.2533, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 980, Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch: 980, Test Loss: 7.2666, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 990, Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch: 990, Test Loss: 7.2757, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Epoch: 1000, Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch: 1000, Test Loss: 7.2802, Test Accuracy: 0.4500\n",
      "-------------\n",
      "Training completed. Best Test Loss: 0.6657, Best Test Accuracy: 0.5500.\n"
     ]
    }
   ],
   "source": [
    "# Define the Transformer model\n",
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_heads, num_classes, num_layers=4):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.embedding = torch.nn.Linear(input_size, input_size)\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(\n",
    "            torch.nn.TransformerEncoderLayer(d_model=input_size, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Transform input dimensions\n",
    "        x = x.unsqueeze(1)  # Add the sequence length dimension, which becomes [batch_size, seq_len, input_size]\n",
    "        x = self.transformer_encoder(x)  # via Transformer encoder\n",
    "        x = x.mean(dim=1)  # Taking the average of the sequences gives a fixed dimension\n",
    "        x = self.fc(x)  # output layer\n",
    "        return x\n",
    "\n",
    "# Initialising the model and optimiser\n",
    "model = TransformerModel(input_size=692, num_heads=4, num_classes=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1E-4)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 1000\n",
    "best_test_loss = float('inf')  # Initialising the optimal test loss\n",
    "best_test_accuracy = 0.0  # Initialising the best test accuracy\n",
    "best_model_path = 'best_transformer_model.pth'  # Path to save the best model\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # training phase\n",
    "    model.train()  # Setting the model to training mode\n",
    "    optimizer.zero_grad()  # Clear the previous gradient\n",
    "    outputs = model(X_train)  # forward pass\n",
    "    loss = criterion(outputs, y_train)  # Calculation of losses\n",
    "    loss.backward()  # backward propagation\n",
    "    optimizer.step()  # Updating parameters\n",
    "\n",
    "    # Checked every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        accuracy = (outputs.argmax(1) == y_train).type(torch.float32).sum().item() / X_train.shape[0]\n",
    "        print(f\"Epoch: {epoch + 1}, Train Loss: {loss.item():.4f}, Train Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # test model\n",
    "        model.eval()  # Setting the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test)  # Forward propagation on the test set\n",
    "            test_loss = criterion(test_outputs, y_test)  # Calculating test losses\n",
    "            test_accuracy = (test_outputs.argmax(1) == y_test).type(torch.float32).sum().item() / X_test.shape[0]  # Calculating Test Accuracy\n",
    "\n",
    "        # Output test results\n",
    "        print(f\"Epoch: {epoch + 1}, Test Loss: {test_loss.item():.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "        print(\"-------------\")\n",
    "\n",
    "        # Preservation of optimal models\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            best_test_accuracy = test_accuracy  # Preservation of optimal accuracy\n",
    "            torch.save(model.state_dict(), best_model_path)  # Preservation of optimal models\n",
    "            print(f\"Saved best model with Test Loss: {best_test_loss:.4f} and Test Accuracy: {best_test_accuracy:.4f}\")\n",
    "\n",
    "# Output the best test loss and accuracy at the end of training\n",
    "print(f\"Training completed. Best Test Loss: {best_test_loss:.4f}, Best Test Accuracy: {best_test_accuracy:.4f}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-25T10:38:31.689468500Z",
     "start_time": "2024-12-25T10:38:16.048179800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7 Conclusions\n",
    "\n",
    "Your conclusions, suggestions for improvements, etc should go here.\n",
    "\n",
    "In this experiment, the training results of the Transformer model and the DNN model were compared. Below, we will analyze the performance of these two models separately and provide suggestions for improvement.\n",
    "\n",
    "### 1. Transformer Model Results Analysis\n",
    "**Training Performance:**\n",
    "Throughout the training process, from the beginning to the end, the training loss gradually decreased, and the training accuracy ultimately reached 1.0000, indicating that the model achieved perfect fitting on the training set.\n",
    "However, the training accuracy exhibited significant fluctuations during the process, suggesting some instability.\n",
    "**Testing Performance:**\n",
    "On the test set, the test loss at the last training epoch was 7.2802, with a test accuracy of 0.4500. This indicates that the model performed very poorly on the test set and failed to generalize well.\n",
    "The best test loss was 0.6657, with a best test accuracy of 0.5500. Although there were best results, the overall accuracy and loss values indicate that the model did not effectively learn valuable features.\n",
    "### 2. DNN Model Results Analysis\n",
    "**Training Performance:**\n",
    "During the training process, the DNN model's training loss gradually decreased, and the final training accuracy also reached 1.0000, which suggests that it also experienced overfitting.\n",
    "**Testing Performance:**\n",
    "The final test loss result was 1.7645, with a test accuracy of 0.4000, demonstrating that the model similarly failed to achieve good generalization on the test set.\n",
    "The best test loss was 0.7171, with a best accuracy of 0.5500, which is similar to the performance of the Transformer model, indicating that both models did not perform well in terms of generalization capabilities.\n",
    "### 3. Summary and Suggestions for Improvement\n",
    "**Conclusions**\n",
    "Overfitting Issue: Both models exhibited a significant disparity in their training set performance, each achieving perfect fitting, indicating that the models are prone to overfitting, lacking the ability to generalize to new data.\n",
    "Instability: The Transformer model, in particular, showed considerable fluctuations in accuracy on the test set, indicating instability in its predictions.\n",
    "**Improvement Suggestions**\n",
    "Introduce Regularization:\n",
    "\n",
    "For both models, consider adding L1 or L2 regularization to limit model complexity.\n",
    "Incorporate Dropout layers in both DNN and Transformer models to reduce reliance on fully connected layers.\n",
    "Adjust Model Architecture:\n",
    "\n",
    "Consider simplifying the model architecture by reducing the number of layers or the number of units per layer, which may enhance the model's ability to generalize.\n",
    "Use a model structure more suitable for audio features, such as Convolutional Neural Networks (CNNs), which are often effective at capturing local features when dealing with audio data.\n",
    "Increase Training Data:\n",
    "\n",
    "If possible, expand the training dataset using data augmentation techniques, such as changing the speed or pitch, to simulate more samples and help the model learn more generalized features.\n",
    "Hyperparameter Optimization:\n",
    "\n",
    "Conduct hyperparameter tuning (e.g., learning rate, batch size) and use cross-validation methods to find the best configuration.\n",
    "Monitor Evaluation Metrics:\n",
    "\n",
    "Continuously monitor performance metrics on the validation set during training, which can help identify overfitting and facilitate early stopping of training when necessary."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8 References\n",
    "\n",
    "Vaswani et al., 2017: \"Attention Is All You Need.\" This seminal paper introduced the Transformer model and its novel self-attention mechanism.\n",
    "Goodfellow et al., 2016: \"Deep Learning.\" This comprehensive book covers foundational concepts in deep learning, including model architectures and training strategies.\n",
    "He et al., 2016: \"Deep Residual Learning for Image Recognition.\" Although focused on image recognition, the residual learning principles can be applied to other domains.\n",
    "Kingma & Ba, 2015: \"Adam: A Method for Stochastic Optimization.\" This paper presents the Adam optimizer, which was used in training the models.\n",
    "Paszke et al., 2019: \"PyTorch: An Imperative Style, High-Performance Deep Learning Library.\" PyTorch was used as the framework for implementing and training the models.\n",
    "Olson et al., 2016: \"Tpot: A Tree-based Pipeline Optimization Tool for Automating Machine Learning.\" TPOT was used for hyperparameter optimization experiments.\n",
    "Chollet, 2018: \"Deep Learning with Python.\" This book provided useful insights into practical implementation of deep learning models using the Keras API.\n",
    "Github Repository: Various open-source resources from GitHub including pre-trained models and benchmark datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
